# mode + seeding
mode: train # or test
seed: 42
episodes: 3000 # number of episodes to run
state_dim: 5
normal_prices: True
safety_test: True

# simulation parameters
time_horizon: 300 # in seconds
trader_time_step: 1.0 # in seconds
initial_inventory: 75 # initial inventory of the trader
actions: [0, 1, 2, 3, 4] # actions: 0 = do nothing, 1 = execute 1 share, 2 = execute 2 shares, etc
history_size: 5 # number of past trader times to consider in the state
final_penalty: 1.0 # reward -= final_penalty * current_inventory (at the end of the episode)
risk_aversion: 0.05 # reward -= risk_aversion * penalty (current inventory, or risk aversion term, during the episode)
alpha_ramp: 15 # ramping parameter for the risk aversion term
exec_security_margin: 1 # security margin for the back-loaded execution agent

# normalization parameters (for NN input)
price_offset: 0.
price_std: 0.1
vol_offset: 5
vol_std: 4

# QRM parameters
theta: 0.6
theta_reinit: 0.85
tick: 0.01
arrival_price: 100.005 # must be a multiple of half tick (0.005 in this case)

# DQN hyperparams
# proba_0: 0.2 # probability of sampling 0 action (do nothing) in the Îµ-greedy policy
epsilon_start: 1.0
epsilon_end: 0.01
epsilon_decay: 0.995
unif_deter_strats: False # whether to use uniformly distribute benchmark strategies (TWAP, ...) during training
prop_greedy_eps: 0.75 # proportion of episodes where the agent is greedy
prop_deter_strats: 0. # proportion of episodes where the agent uses the benchmark strategies (TWAP, Back Load, etc)

gamma: 0.99
learning_rate: 0.001
alpha: 0.95
eps: 0.01
batch_size: 64
memory_capacity: 10000
warmup_steps: 64
target_update_freq: 3000
target_update_freq_2: 500
dynamic_lr: False
dynamic_batch_size: False

# environment
logging_every: 5 # print reward in the terminal every `logging_every` episodes during training
folder_path_intensity_table: "calibration_data/intensity_table/"
folder_path_invariant: "calibration_data/invariant_distribution/"
