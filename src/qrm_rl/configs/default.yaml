# Training parameters
agent_type: "DQN" # "PPO"
episodes: 260000 #80000

# Simulation parameters
time_horizon: 600 # 78 # 21 # 156 # 78
trader_time_step: 25 # 6
initial_inventory: 25 # 25 # 30 # 42
final_penalty: 1
risk_aversion: 0.

# State space
basic_state: True
len_basic_state: 5
len_state_lob: 3
history_size: 1

# Action space
actions: [0, 1.0] # [0, 0.25, 0.5, 0.75]

# Parameters
gamma: 1.0
learning_rate: 0.0001
n_steps: 5 # 13

# DQN parameters
train_freq: 1
gradient_steps: 1
batch_size: 1024
learning_starts: 1024
buffer_size: 300000
target_update_interval: 2000 # 350

# Epsilon greedy
exploration_initial_eps: 1.0
exploration_final_eps: 0.01 # 0.0
exploration_fraction: 0.03 # 0.1

# Normalization parameters
price_offset: 0.0177 # 0.025 # 0.
price_std: 0.0217 # 0.05 # 0.3
vol_offset: 8.356 # 2.133 # 8.6 # 5
vol_std: 6.446 # 0.831 # 6.9 # 4

# QRM parameters
theta: 0.7
theta_reinit: 0.85
tick: 0.01
arrival_price: 100.005

# Other
max_events_per_second: 20
aes: [836, 1068, 1069]
mode: train
seed: 42
logging: True
logging_every: 100
folder_path_intensity_table: "calibration_data/intensity_table/"
folder_path_invariant: "calibration_data/invariant_distribution/"
