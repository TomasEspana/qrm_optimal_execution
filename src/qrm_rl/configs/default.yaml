# Training parameters
agent_type: "DDQN"
episodes: 260000 #260000
max_episodes: 1000000

# Simulation parameters
time_horizon: 600 
trader_time_step: 25 
initial_inventory: 25
final_penalty: 1
risk_aversion: 0.

# State space
basic_state: True
len_basic_state: 5
len_state_lob: 3
history_size: 1

# Action space
actions: [0, 1.0]

# Parameters
gamma: 0.995
learning_rate: 0.0001
n_steps: 1 

# DQN parameters
train_freq: 100 
gradient_steps: 1
batch_size: 1024
learning_starts: 40000 
buffer_size: 1000000
target_update_interval: 1000

# Epsilon greedy
exploration_initial_eps: 1.0
exploration_final_eps: 0.01 
exploration_fraction: 0.03

# Normalization parameters (approximation from simulations)
price_offset: 0.0177
price_std: 0.0217 
vol_offset: 8.356 
vol_std: 6.446

# QRM parameters
theta: 0.7
theta_reinit: 0.85
tick: 0.01
arrival_price: 100.005

# Other
max_events_per_second: 20
aes: [836, 1068, 1069]
mode: train
seed: 42
logging: True
logging_every: 100
folder_path_intensity_table: "calibration_data/"
folder_path_invariant: "calibration_data/"
