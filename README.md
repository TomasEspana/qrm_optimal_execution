# RL for Optimal Execution in Queue-Reactive Models

Code used in our paper: [![arXiv](https://img.shields.io/badge/arXiv-2511.15262-b31b1b.svg)](https://arxiv.org/abs/2511.15262) T. Espana, Y. Hafsi, F. Lillo, E. Vittori (2025).

## ðŸ“„ Abstract

We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.

## ðŸš€ Setup Instructions

### 1. Clone the repo

```bash
git clone https://github.com/espanato/qrm_optimal_execution.git
cd qrm_optimal_execution
```

### 2. Virtual env and dependencies

```bash
python -m venv .venv
# macOS / Linux
source .venv/bin/activate
# Windows (PowerShell)
# .\.venv\Scripts\activate
python -m pip install --upgrade pip
pip install -r requirements.txt
python -m pip install -e .
```

### 3. See the *quick guide* below and have fun!


## ðŸ“Š The Queue-Reactive Model 

The implemented Queue-Reactive Model corresponds to Model I of the paper: [![arXiv](https://img.shields.io/badge/arXiv-1312.0563-b31b1b.svg)](https://arxiv.org/abs/1312.0563)

### 1. Quick Guide 

See `notebooks/main.ipynb` (Section I) for a step-by-step walkthrough of running QRM LOB simulations, as illustrated below.

<p align="center">
  <img src="img/lob_simulation.png" alt="Figure caption" width="600">
</p>

### LOB (DataFrame) â€” how to read it?

Each row is **one LOB event** generated by the QRM, together with the **state of the book *after* applying the event**.

<ins>**Columns:**</ins>
- `time`: event time (in seconds).
- `p_mid`: mid-price.
- `p_ref`: reference price (see QRM paper for more details).
- `side`: which side the event acts on (`bid` / `ask`).
- `depth`: price level **relative to `p_ref`** on that side (`1` = 0.5 ticks away from `p_ref`, `2` = 1.5 ticks away from `p_ref`, â€¦).
- `event`: event type (`limit`, `cancel`, `market`).
- `redrawn`: whether the volumes were sampled from the invariant distribution or not.

<ins>**Queue Sizes:**</ins>
- `q_bid1`, `q_bid2`, `q_bid3`: sizes of the bid queues at depths 1/2/3 (relatively to `p_ref`).
- `q_ask1`, `q_ask2`, `q_ask3`: sizes of the ask queues at depths 1/2/3 (relatively to `p_ref`).

The queue sizes are expressed in Average Event Size (AES) units (see QRM paper for more details).

<ins>**Example:**</ins>

In the red-highlighted row, we have an `ask` **cancel** at `depth = 1` occurring at `time = 0.006905`.
You can verify its effect by comparing the queue sizes with the previous row: `q_ask1` decreases from `3` to `2`, while other queues (and prices) remain unchanged.


### 2. How to use your own calibrated intensities

<ins>**Intensity Table:**</ins>

Save the intensities $`\lambda_i^{\{L, C, M\}}(q_i)`$ in a numpy array at `calibration_data/intensity_table.npy` of shape $`(K, Q+1, 2, 3)`$ where $K$ is the depth of the LOB (here, $K=3$); $Q$ the maximum queue size (here, $Q=50$); shape `2` is for the book sides (1:`bid` and 2:`ask`) and shape `3` is for the event types (1: `limit`, 2: `cancel`, 3: `market`). For more details, see `src/qrm_core/intensity.py`.

<ins>**Invariant Distribution:**</ins>

Run the function in `src/qrm_core/invariant_distribution.py` which will create and save the `.npy` file of the invariant distribution in the folder `calibration_data`. 

<ins>Note:</ins> We assume bidâ€“ask symmetry for the intensities. The code also supports the asymmetric case; in that setting, you must generate two invariant-distribution files (one for bids and one for asks).


## ðŸ¤– Reinforcement Learning

This repo trains a **Double Deep Q-Network (DDQN)** trading agent using **Stable-Baselines3 `DQN`**. The model is built in `_build_dqn()` (`src/qrm_rl/runner.py`).

### Environment and actions
The agent acts on an **evenly spaced trading schedule** over the horizon $[0, T]$. At each decision time, it selects an action that translates into a **buy quantity proportional to the available volume at the best ask**, with the objective of buying a total inventory $X_0$. The agent **only buys** (no selling, no limit-order placement); see `step()` (`src/qrm_rl/market_environment.py`).

### Configuration
All experiment parameters (time horizon, initial inventory, etc.) are in:
- `src/qrm_rl/configs/default.yaml`

The default configuration matches the paper settings (Section 5).

### Reproducing the results
1) **Train**  
Run `scripts/run_train.py`.  
This trains the DDQN agent and saves the SB3 model as `save_model/ddqn_{run_id}.zip`. The script also outputs feature-importance diagnostics (input gradients and SHAP).

2) **Test**  
Edit `scripts/run_test.py` and set `train_run_id` to the desired run id, then run it. Test outputs are saved as `.pkl` files in `test_data/`.

### Extending the market environment
Most environment logic lives in `src/qrm_rl/market_environment.py` (state construction, action-to-order mapping, reward, and episode termination). To extend or modify the environment, start there. For example, to add more state features (e.g., a longer LOB history), edit `get_state()` (`src/qrm_rl/market_environment.py`).
